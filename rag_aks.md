[
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#jump-
content)

Jump to content

[ ](https://statics.teams.cdn.office.net/)

arrow_back

# Deploy RAG application using NVIDIA NIMs and NeMo Retriever on Google
Kubernetes Engine

share

[ post_linkedin Share on LinkedIn Feed
](https://www.linkedin.com/sharing/share-
offsite/?url=https%3A%2F%2Fexplore.qwiklabs.com%2Fcatalog_lab%2F18317%3Futm_medium%3Dsocial%26utm_source%3Dlinkedin%26utm_campaign%3Dql-
social-share) Share on LinkedIn Feed [ post_twitter Twitter
](https://twitter.com/intent/tweet?text=Learn%20cloud%20tech%20through%20hands-
on%20training%20on%20%23Qwiklabs%21&url=https%3A%2F%2Fexplore.qwiklabs.com%2Fcatalog_lab%2F18317%3Futm_medium%3Dsocial%26utm_source%3Dtwitter%26utm_campaign%3Dql-
social-share&hashtags=) Twitter [ post_facebook Facebook
](https://facebook.com/sharer.php?display=popup&u=https%3A%2F%2Fexplore.qwiklabs.com%2Fcatalog_lab%2F18317%3Futm_medium%3Dsocial%26utm_source%3Dfacebook%26utm_campaign%3Dql-
social-share) Facebook Share Link

content_copy

[
](https://explore.qwiklabs.com/favorite.json?item_id=7867&item_type=CatalogItem)

favorite_border

dashboard

help_outline

Help Center

[ Email support ](mailto:labs-for-sales-support@google.com)

Chat support

[ Report Illegal Content
](https://reportingwidget.google.com/widget/54?cid=1&url=https://explore.qwiklabs.com/focuses/7867?)

language

[ العربية‬‎
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=ar&parent=catalog&search_id=26820)
[ Deutsch
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=de&parent=catalog&search_id=26820)
[ English
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=en&parent=catalog&search_id=26820)
[ español (Latinoamérica)
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=es&parent=catalog&search_id=26820)
[ français
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=fr&parent=catalog&search_id=26820)
[ français (Canada)
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=fr_CA&parent=catalog&search_id=26820)
[ עברית
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=he&parent=catalog&search_id=26820)
[ bahasa Indonesia
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=id&parent=catalog&search_id=26820)
[ italiano
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=it&parent=catalog&search_id=26820)
[ 日本語
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=ja&parent=catalog&search_id=26820)
[ 한국어
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=ko&parent=catalog&search_id=26820)
[ polski
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=pl&parent=catalog&search_id=26820)
[ português (Brasil)
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=pt_BR&parent=catalog&search_id=26820)
[ português (Portugal)
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=pt_PT&parent=catalog&search_id=26820)
[ русский
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=ru&parent=catalog&search_id=26820)
[ Türkçe
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=tr&parent=catalog&search_id=26820)
[ українська
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=uk&parent=catalog&search_id=26820)
[ 简体中文
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=zh&parent=catalog&search_id=26820)
[ 繁體中文
](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&locale=zh_TW&parent=catalog&search_id=26820)
[ star 0 pts
![](https://cdn.qwiklabs.com/assets/leagues/bronze_icon-e39312c8973c998da7bde8ed32e96ee1c8813eee.png)
](https://explore.qwiklabs.com/profile/leaderboard)

#### Alexander Zeltov US

azeltov@nvidia.com

[ 0 Credits  ](https://explore.qwiklabs.com/my_account/payments)

[Settings](https://explore.qwiklabs.com/my_account/profile)

Sign Out

[Privacy](https://explore.qwiklabs.com/privacy_policy) ·
[Terms](https://explore.qwiklabs.com/terms_of_service)

![Google Labs for
Sales](./ragk_aks_files/f1M7GsoamHN2p_DqAfS+13QyL5O0OJgnE68v7m2VetE=)

[ home Home ](https://explore.qwiklabs.com/) [ school Catalog
](https://explore.qwiklabs.com/catalog) [ event_note Profile
](https://explore.qwiklabs.com/profile)

Start Lab  05:00:00

clear

#  Deploy RAG application using NVIDIA NIMs and NeMo Retriever on Google
Kubernetes Engine

schedule 5 hours  universal_currency_alt No cost

[ ](https://explore.qwiklabs.com/focuses/7867/reviews?parent=catalog)

Rate Lab

info This lab may incorporate AI tools to support your learning.

Lab instructions and tasks

expand_less

  * [ISV026](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step1)
  * [Introduction](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step2)
  * [What you will learn](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step3)
  * [Learn the Components](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step4)
  * [Setup and Requirements](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step5)
  * [Task 1. Infrastructure Deployment](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step6)
  * [Task 2. Configure NVIDIA NGC API Key](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step7)
  * [Task 3. Deploying NVIDIA NIM](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step8)
  * [Task 4. Deploying NeMo Retriever Embedding Microservice](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step9)
  * [Task 5. Deploying NeMo Retriever Ranking Microservice](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step10)
  * [Task 5. Deploying Milvus](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step11)
  * [Task 6. Deploying Chain Server & RAG Playground](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step12)
  * [Task 7. Access the RAG Playground Frontend Service](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step13)
  * [Congratulations!](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step14)

## ISV026

![nim](./ragk_aks_files/sK7UN6tU8mU7zZJRiCHwKztIXym0koByOMuGzEfXWXU=)

## Introduction

This workshop will guide you through deploying a [Retrieval Augmented
Generation (RAG) text Q&A agent
microservice](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/aiworkflows/helm-
charts/rag-app-text-chatbot-langchain) on Google Kubernetes Engine (GKE).
You'll leverage the power of NVIDIA Inference Microservices (NIMs) and Nemo
LLM Retriever Embedding to build a robust question-answering system. This
system utilizes Milvus as the vector store to manage embeddings and generate
accurate responses to user queries.

This workshop is ideal for developers and data scientists interested in:

  * Building RAG applications: Learn how to construct a complete RAG pipeline using NVIDIA's pre-built microservices and open-source tools.
  * Optimizing LLM inference: Explore how to deploy and utilize TensorRT optimized LLMs for efficient inference within a microservice architecture.
  * Leveraging vector databases: Understand how to use Milvus to store and query embeddings for semantic search in a RAG workflow.

## What you will learn

By the end of this workshop, you will have hands-on experience with:

  1. Deploying a RAG pipeline on GKE: Learn to deploy a complete RAG pipeline, including LLM, embedding, and retriever microservices, onto your GKE cluster using NVIDIA NIMs.
  2. Integrating with Milvus vector database: Understand how to connect your RAG pipeline to a Milvus vector store for efficient storage and retrieval of embeddings.
  3. Utilizing the NVIDIA Langchain wrapper: Gain familiarity with the NVIDIA Langchain wrapper for seamless interaction with deployed NIMs.
  4. Managing and scaling your RAG deployment: Explore techniques for managing, monitoring, and scaling your RAG pipeline using Kubernetes features to ensure optimal performance and resource utilization.

## Learn the Components

### **GPUs in Google Kubernetes Engine (GKE)**

[GPUs](https://cloud.google.com/compute/docs/gpus) let you accelerate specific
workloads running on your nodes such as machine learning and data processing.
GKE provides a range of machine type options for node configuration, including
machine types with [NVIDIA H100, L4, and A100
GPUs](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus).

### **NVIDIA NIMs**

[NVIDIA NIMs](https://www.nvidia.com/en-us/ai/) are a set of easy-to-use
inference microservices for accelerating the deployment of foundation models
on any cloud or data center and helping to keep your data secure.

### **NVIDIA NeMo Retriever Embedding & Reranker Microservice**

[NVIDIA NeMo Retriever](https://developer.nvidia.com/blog/develop-production-
grade-text-retrieval-pipelines-for-rag-with-nvidia-nemo-retriever), part of
[NVIDIA NeMo](https://www.nvidia.com/en-us/ai-data-science/products/nemo/), is
a collection of generative AI microservices that enable organizations to
seamlessly connect custom models to diverse business data and deliver highly
accurate responses.

### **NVIDIA AI Enterprise**

[NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-
enterprise/) is an end-to-end, cloud-native software platform that accelerates
data science pipelines and streamlines development and deployment of
production-grade co-pilots and other generative AI applications. Available
through [GCP
Marketplace](https://console.cloud.google.com/marketplace/product/nvidia/nvidia-
ai-enterprise-vmi).

### **Chain Server**

NVIDIA developed a chain server that communicates with the inference server.
The server also supports retrieving embeddings from the vector database before
submitting a query to the inference server to perform retrieval augmented
generation.

### **Vector Database**

The Chain Server supports connecting to either Milvus or pgvector. NVIDIA
provides a sample RAG pipeline that deploys Milvus to simplify demonstrating
the inference and embedding microservices.

### **RAG Playground**

The application provides a user interface for entering queries that are
answered by the inference microservice. The application also supports
uploading documents that the embedding microservice processes and stores as
embeddings in a vector database.

## Setup and Requirements

#### Before you click the Start Lab button

Read these instructions. Labs are timed and you cannot pause them. The timer,
which starts when you click **Start Lab** , shows how long Google Cloud
resources will be made available to you.

This Qwiklabs hands-on lab lets you do the lab activities yourself in a real
cloud environment, not in a simulation or demo environment. It does so by
giving you new, temporary credentials that you use to sign in and access
Google Cloud for the duration of the lab.

#### What you need

To complete this lab, you need:

  * Access to a standard internet browser (Chrome browser recommended).
  * Time to complete the lab.

**Note:** If you already have your own personal Google Cloud account or
project, do not use it for this lab.

**Note:** If you are using a Pixelbook, open an Incognito window to run this
lab.

#### How to start your lab and sign in to the Google Cloud Console

  1. Click the **Start Lab** button. If you need to pay for the lab, a pop-up opens for you to select your payment method. On the left is a panel populated with the temporary credentials that you must use for this lab.

![Open Google
Console](./ragk_aks_files/_tHp4GI5VSDyTtdqi3qDFtevuY014F88+Fow_adnRgE=)

  2. Copy the username, and then click **Open Google Console**. The lab spins up resources, and then opens another tab that shows the **Sign in** page.

![Sign in](./ragk_aks_files/VkUIAFY2xX3zoHgmWqYKccRLwFrR4BfARLd5ojmlbhs=)

**_Tip:_** Open the tabs in separate windows, side-by-side.

If you see the **Choose an account** page, click **Use Another Account**.  
![Choose an
account](./ragk_aks_files/eQ6xPnPn13GjiJP3RWlHWwiMjhooHxTNvzfg1AL2WPw=)

  3. In the **Sign in** page, paste the username that you copied from the Connection Details panel. Then copy and paste the password.

**_Important:_** You must use the credentials from the Connection Details
panel. Do not use your Qwiklabs credentials. If you have your own Google Cloud
account, do not use it for this lab (avoids incurring charges).

  4. Click through the subsequent pages:

     * Accept the terms and conditions.
     * Do not add recovery options or two-factor authentication (because this is a temporary account).
     * Do not sign up for free trials.

After a few moments, the Cloud Console opens in this tab.

**Note:** You can view the menu with a list of Google Cloud Products and
Services by clicking the **Navigation menu** at the top-left. ![Cloud Console
Menu](./ragk_aks_files/9vT7xPlxoNP_PsK0J8j0ZPFB4HnnpaIJVCDByaBrSHg=)

### Activate Cloud Shell

Cloud Shell is a virtual machine that is loaded with development tools. It
offers a persistent 5GB home directory and runs on the Google Cloud. Cloud
Shell provides command-line access to your Google Cloud resources.

In the Cloud Console, in the top right toolbar, click the **Activate Cloud
Shell** button.

![Cloud Shell
icon](./ragk_aks_files/vdY5e_an9ZGXw5a_ZMb1agpXhRGozsOadHURcR8thAQ=)

Click **Continue**.

![cloudshell_continue.png](./ragk_aks_files/lr3PBRjWIrJ+MQnE8kCkOnRQQVgJnWSg4UWk16f0s_A=)

It takes a few moments to provision and connect to the environment. When you
are connected, you are already authenticated, and the project is set to your
_PROJECT_ID_. For example:

![Cloud Shell
Terminal](./ragk_aks_files/hmMK0W41Txk+20bQyuDP9g60vCdBajIS+52iI2f4bYk=)

`gcloud` is the command-line tool for Google Cloud. It comes pre-installed on
Cloud Shell and supports tab-completion.

You can list the active account name with this command:

    
    
    gcloud auth list

Copied!

content_copy

gcloud auth list

(Output)

    
    
    Credentialed accounts:
     - <myaccount>@<mydomain>.com (active)

Credentialed accounts: \- <myaccount>@<mydomain>.com (active)

(Example output)

    
    
    Credentialed accounts:
     - google1623327_student@qwiklabs.net

Credentialed accounts: \- google1623327_student@qwiklabs.net

You can list the project ID with this command:

    
    
    gcloud config list project

Copied!

content_copy

gcloud config list project

(Output)

    
    
    [core]
    project = <project_ID>

[core] project = <project_ID>

(Example output)

    
    
    [core]
    project = qwiklabs-gcp-44776a13dea667a6

[core] project = qwiklabs-gcp-44776a13dea667a6  For full documentation of
`gcloud` see the [gcloud command-line tool
overview](https://cloud.google.com/sdk/gcloud).

## Task 1. Infrastructure Deployment

  1. Open **Cloud Shell**

  2. Specify the following parameters
    
        export PROJECT_ID="Inserted at lab startup"
    export REGION=
    export ZONE=
    export CLUSTER_NAME=nim-operator-demo
    export NODE_POOL_MACHINE_TYPE=g2-standard-48
    export CLUSTER_MACHINE_TYPE=e2-standard-4
    export GPU_TYPE=nvidia-l4
    export GPU_COUNT=4
    export WORKLOAD_POOL="Inserted at lab startup".svc.id.goog

Copied!

content_copy

export PROJECT_ID={{{ project_0.project_id | "Inserted at lab startup" }}} export REGION={{{ project_0.default_region }}} export ZONE={{{ project_0.default_zone }}} export CLUSTER_NAME=nim-operator-demo export NODE_POOL_MACHINE_TYPE=g2-standard-48 export CLUSTER_MACHINE_TYPE=e2-standard-4 export GPU_TYPE=nvidia-l4 export GPU_COUNT=4 export WORKLOAD_POOL={{{ project_0.project_id | "Inserted at lab startup" }}}.svc.id.goog 
  3. Create GKE Cluster
    
        gcloud container clusters create ${CLUSTER_NAME} \
        --project=${PROJECT_ID} \
        --location=${ZONE} \
        --workload-pool=${WORKLOAD_POOL} \
        --release-channel=rapid \
        --machine-type=${CLUSTER_MACHINE_TYPE} \
        --num-nodes=1

Copied!

content_copy

gcloud container clusters create ${CLUSTER_NAME} \ \--project=${PROJECT_ID} \
\--location=${ZONE} \ \--workload-pool=${WORKLOAD_POOL} \ \--release-
channel=rapid \ \--machine-type=${CLUSTER_MACHINE_TYPE} \ \--num-nodes=1

  4. Create GPU node pool
    
        gcloud container node-pools create gpupool \
        --accelerator type=${GPU_TYPE},count=${GPU_COUNT},gpu-driver-version=latest \
        --project=${PROJECT_ID} \
        --location=${ZONE} \
        --cluster=${CLUSTER_NAME} \
        --machine-type=${NODE_POOL_MACHINE_TYPE} \
        --num-nodes=1

Copied!

content_copy

gcloud container node-pools create gpupool \ \--accelerator
type=${GPU_TYPE},count=${GPU_COUNT},gpu-driver-version=latest \
\--project=${PROJECT_ID} \ \--location=${ZONE} \ \--cluster=${CLUSTER_NAME} \
\--machine-type=${NODE_POOL_MACHINE_TYPE} \ \--num-nodes=1

## Task 2. Configure NVIDIA NGC API Key

The first step in the pipeline is to deploy an inference NIM - this will be
used to host the LLM that will be responsible for generating responses for a
given set of user queries. In order to get started with NIM, we’ll need to
make sure we have access to an [NGC API
key](https://org.ngc.nvidia.com/setup/api-key). We can export this key to be
used an environment variable:

    
    
    export NGC_CLI_API_KEY="<YOUR NGC API KEY>"

Copied!

content_copy

export NGC_CLI_API_KEY="<YOUR NGC API KEY>"

## Task 3. Deploying NVIDIA NIM

  1. Open **Cloud Shell**

  2. Fetch NIM LLM Helm Chart Once we’ve set the NGC API key, we’ll need to fetch the NIM LLM Helm chart from NGC:
    
        helm fetch https://helm.ngc.nvidia.com/nim/charts/nim-llm-1.1.2.tgz --username='$oauthtoken' --password=$NGC_CLI_API_KEY

Copied!

content_copy

helm fetch https://helm.ngc.nvidia.com/nim/charts/nim-llm-1.1.2.tgz
--username='$oauthtoken' --password=$NGC_CLI_API_KEY

  3. Create a NIM Namespace Namespaces are used to manage resources for a specific service or set of services in kubernetes. It’s best practice to ensure all the resources for a given service are managed in its corresponding namespace. We will set up the namespaces for the other corresponding services in later steps, but we create a namespace for our NIM service using the following kubectl command:
    
        # create namespace
    kubectl create namespace nim

Copied!

content_copy

# create namespace kubectl create namespace nim

  4. Configure secrets In order to configure and launch an NVIDIA NIM, it is important to configure the secrets we’ll need to pull all the model artifacts directly from NGC. This can be done using your NGC API key:
    
        kubectl create secret docker-registry registry-secret --docker-server=nvcr.io --docker-username='$oauthtoken'     --docker-password=$NGC_CLI_API_KEY -n nim
    
    kubectl create secret generic ngc-api --from-literal=NGC_API_KEY=$NGC_CLI_API_KEY -n nim

Copied!

content_copy

kubectl create secret docker-registry registry-secret --docker-server=nvcr.io
--docker-username='$oauthtoken' --docker-password=$NGC_CLI_API_KEY -n nim
kubectl create secret generic ngc-api --from-
literal=NGC_API_KEY=$NGC_CLI_API_KEY -n nim

  5. Setup NIM Configuration We deploy the LLama 3 8B instruct NIM for this exercise. In order to configure our NIM, we create a custom value file where we configure the deployment:
    
        # create nim_custom_value.yaml manifest
    cat <<EOF > nim_custom_value.yaml
    image:
      repository: "nvcr.io/nim/meta/llama3-8b-instruct" # container location
      tag: 1.0.0 # NIM version you want to deploy
    model:
      ngcAPISecret: ngc-api  # name of a secret in the cluster that includes a key named NGC_CLI_API_KEY and is an NGC API key
    persistence:
      enabled: true
    imagePullSecrets:
      - name: registry-secret # name of a secret used to pull nvcr.io images, see https://kubernetes.io/docs/tasks/    configure-pod-container/pull-image-private-registry/
    EOF

Copied!

content_copy

# create nim_custom_value.yaml manifest cat <<EOF > nim_custom_value.yaml
image: repository: "nvcr.io/nim/meta/llama3-8b-instruct" # container location
tag: 1.0.0 # NIM version you want to deploy model: ngcAPISecret: ngc-api #
name of a secret in the cluster that includes a key named NGC_CLI_API_KEY and
is an NGC API key persistence: enabled: true imagePullSecrets: \- name:
registry-secret # name of a secret used to pull nvcr.io images, see
https://kubernetes.io/docs/tasks/ configure-pod-container/pull-image-private-
registry/ EOF

  6. Launching NIM deployment Now we can deploy our NIM microservice to the namespace we created:
    
        helm install my-nim nim-llm-1.1.2.tgz -f nim_custom_value.yaml --namespace nim

Copied!

content_copy

helm install my-nim nim-llm-1.1.2.tgz -f nim_custom_value.yaml --namespace nim

Verify NIM pod is running

    
        kubectl get pods -n nim

Copied!

content_copy

kubectl get pods -n nim

  7. Testing NIM deployment Once we’ve verified that our NIM service was deployed successfully. We can make inference requests to see what type of feedback we’ll receive from the NIM service. In order to do this, we enable port forwarding on the service to be able to access the NIM from our localhost on port 8000:
    
        kubectl port-forward service/my-nim-nim-llm 8000:8000 -n nim

Copied!

content_copy

kubectl port-forward service/my-nim-nim-llm 8000:8000 -n nim

Next, we can open another terminal or tab in the cloud shell and try the
following request:

    
        curl -X 'POST' \
      'http://localhost:8000/v1/chat/completions' \
      -H 'accept: application/json' \
      -H 'Content-Type: application/json' \
      -d '{
      "messages": [
        {
          "content": "You are a polite and respectful chatbot helping people plan a vacation.",
          "role": "system"
        },
        {
          "content": "What should I do for a 4 day vacation in Spain?",
          "role": "user"
        }
      ],
      "model": "meta/llama3-8b-instruct",
      "max_tokens": 128,
      "top_p": 1,
      "n": 1,
      "stream": false,
      "stop": "\n",
      "frequency_penalty": 0.0
    }'

Copied!

content_copy

curl -X 'POST' \ 'http://localhost:8000/v1/chat/completions' \ -H 'accept:
application/json' \ -H 'Content-Type: application/json' \ -d '{ "messages": [
{ "content": "You are a polite and respectful chatbot helping people plan a
vacation.", "role": "system" }, { "content": "What should I do for a 4 day
vacation in Spain?", "role": "user" } ], "model": "meta/llama3-8b-instruct",
"max_tokens": 128, "top_p": 1, "n": 1, "stream": false, "stop": "\n",
"frequency_penalty": 0.0 }'

If you get a chat completion from the NIM service, that means the service is
working as expected! From here, we can move on to deploying the NeMo Retriever
service.

## Task 4. Deploying NeMo Retriever Embedding Microservice

  1. Fetch NeMo Retriever Embedding Helm Chart The NeMo Retriever microservice can be installed via Helm. As a starting point we can fetch the Helm chart, assuming we still leverage the NGC API Key we configured earlier:
    
        helm fetch https://helm.ngc.nvidia.com/nim/nvidia/charts/text-embedding-nim-1.2.0.tgz --username='$oauthtoken'     --password=$NGC_CLI_API_KEY

Copied!

content_copy

helm fetch https://helm.ngc.nvidia.com/nim/nvidia/charts/text-embedding-
nim-1.2.0.tgz --username='$oauthtoken' --password=$NGC_CLI_API_KEY

  2. Create NeMo Retriever Namespace We create the NeMo Retriever namespace to manage all the kubernetes related resource for the microservice:
    
        kubectl create namespace nrem

Copied!

content_copy

kubectl create namespace nrem

  3. Creating Secrets We need to configure image pull secrets and NGC secrets to enable the pulling of model artifacts from NGC and the NVCR registry. In the next following steps, we configure these secrets using the API key we set as a prerequisite:
    
        # set secrets
    
    DOCKER_CONFIG='{"auths":{"nvcr.io":{"username":"$oauthtoken", "password":"'${NGC_CLI_API_KEY}'" }}}'
    echo -n $DOCKER_CONFIG | base64 -w0
    NGC_REGISTRY_PASSWORD=$(echo -n $DOCKER_CONFIG | base64 -w0 )
    
    # build imagepull.yaml file
    cat <<EOF > imagepull.yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: nvcrimagepullsecret
    type: kubernetes.io/dockerconfigjson
    data:
      .dockerconfigjson: ${NGC_REGISTRY_PASSWORD}
    EOF
    
    # build ngc cli api secrets manifest
    cat <<EOF > ngc-cli.yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: ngc-api
    type: Opaque
    data:
      NGC_API_KEY: $(echo -n $NGC_CLI_API_KEY | base64 -w0 )
    EOF
    
    # apply manifests and create secrets 
    kubectl apply -n nrem -f imagepull.yaml
    kubectl apply -n nrem -f ngc-cli.yaml

Copied!

content_copy

# set secrets DOCKER_CONFIG='{"auths":{"nvcr.io":{"username":"$oauthtoken", "password":"'${NGC_CLI_API_KEY}'" }}}' echo -n $DOCKER_CONFIG | base64 -w0 NGC_REGISTRY_PASSWORD=$(echo -n $DOCKER_CONFIG | base64 -w0 ) # build imagepull.yaml file cat <<EOF > imagepull.yaml apiVersion: v1 kind: Secret metadata: name: nvcrimagepullsecret type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: ${NGC_REGISTRY_PASSWORD} EOF # build ngc cli api secrets manifest cat <<EOF > ngc-cli.yaml apiVersion: v1 kind: Secret metadata: name: ngc-api type: Opaque data: NGC_API_KEY: $(echo -n $NGC_CLI_API_KEY | base64 -w0 ) EOF # apply manifests and create secrets kubectl apply -n nrem -f imagepull.yaml kubectl apply -n nrem -f ngc-cli.yaml 
  4. Install the Helm Chart for Nemo Retriever Embedding Microservice Now that we’ve configured the secrets we need to pull artifacts from NGC, we can deploy the NeMo retriever microservice via Helm. In this case, we deploy the service using the NVIDIA Retrieval QA E5 Embedding v5 NIM:
    
        helm upgrade --install \
      --namespace nrem \
      --set image.repository=nvcr.io/nim/nvidia/nv-embedqa-e5-v5 \
      --set image.tag=1.1.0 \
      nemo-embedder \
      text-embedding-nim-1.2.0.tgz

Copied!

content_copy

helm upgrade --install \ \--namespace nrem \ \--set
image.repository=nvcr.io/nim/nvidia/nv-embedqa-e5-v5 \ \--set image.tag=1.1.0
\ nemo-embedder \ text-embedding-nim-1.2.0.tgz

Ensure all of the pods are in a running state:

    
        kubectl get pods -n nrem

Copied!

content_copy

kubectl get pods -n nrem

The output should be similar to the following:

    
        NAME                                          READY   STATUS    RESTARTS   AGE
    nemo-embedder-text-embedding-nim-0   1/1     Running   0          10m

Copied!

content_copy

NAME READY STATUS RESTARTS AGE nemo-embedder-text-embedding-nim-0 1/1 Running
0 10m

Once we’ve verified the pods are running. We can test by enabling port
forwarding on the service to be able to access from our localhost:

    
        kubectl port-forward -n nrem service/nemo-embedder-text-embedding-nim 8000:8000

Copied!

content_copy

kubectl port-forward -n nrem service/nemo-embedder-text-embedding-nim
8000:8000

Next, we can open another terminal or tab in the cloud shell and try the
following request:

    
        curl -X 'POST'   'http://localhost:8000/v1/embeddings'   -H 'accept: application/json'   -H 'Content-Type: application/json'       -d '{
        "input": "hello world",
        "model": "nvidia/nv-embedqa-e5-v5",
        "input_type": "passage"
      }'

Copied!

content_copy

curl -X 'POST' 'http://localhost:8000/v1/embeddings' -H 'accept:
application/json' -H 'Content-Type: application/json' -d '{ "input": "hello
world", "model": "nvidia/nv-embedqa-e5-v5", "input_type": "passage" }'

We should receive an embedding vector as output from the cURL command. Once
we’ve verified that we’re able to ping the service, ctrl + c to stop port
forwarding. From here, we can move on to setting up the Milvus microservice.

## Task 5. Deploying NeMo Retriever Ranking Microservice

In this section, we will deploy the NeMo Retriever Ranking microservice. In
the previous section, we built the namespace and secrets we’ll need to run
this service! We just have to fetch the Helm chart from NGC and install it in
our current workspace.

  1. Fetch NeMo Retriever Reranking Helm Chart

The NeMo Retriever microservice can be installed via Helm. As a starting point
we can fetch the Helm chart, assuming we still leverage the NGC API Key we
configured earlier:

    
        helm fetch https://helm.ngc.nvidia.com/nim/nvidia/charts/text-reranking-nim-1.3.0.tgz --username='$oauthtoken'     --password=$NGC_CLI_API_KEY

Copied!

content_copy

helm fetch https://helm.ngc.nvidia.com/nim/nvidia/charts/text-reranking-
nim-1.3.0.tgz --username='$oauthtoken' --password=$NGC_CLI_API_KEY

  2. Configure secrets

We need to configure the secrets we’ll need to pull all the model artifacts
directly from NGC. This can be done using your NGC API key:

    
        kubectl create secret -n nrem docker-registry ngc-secret --docker-server=nvcr.io --docker-username='$oauthtoken'     --docker-password=$NGC_CLI_API_KEY

Copied!

content_copy

kubectl create secret -n nrem docker-registry ngc-secret --docker-
server=nvcr.io --docker-username='$oauthtoken' --docker-
password=$NGC_CLI_API_KEY

  3. Install the Helm Chart for Nemo Retriever Ranking Microservice

Now that we’ve been able to fetch the Helm chart, we can deploy the
microservice by running the following command:

  
**Please note** : When deploying in your own environment, **do not use** the
`--set containerSecurityContext.runAsUser=0` flag - this is used only in this
Qwiklab environment

    
          helm upgrade --install \
      --namespace nrem \
      --set image.repository=nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2 \
      --set image.tag=1.3.1 \
      --set containerSecurityContext.runAsUser=0 \
      nemo-ranker \
      text-reranking-nim-1.3.0.tgz 

Copied!

content_copy

helm upgrade --install \ \--namespace nrem \ \--set
image.repository=nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2 \ \--set
image.tag=1.3.1 \ \--set containerSecurityContext.runAsUser=0 \ nemo-ranker \
text-reranking-nim-1.3.0.tgz

We can verify the pods status by running the command below:

    
        kubectl get pods -n nrem

Copied!

content_copy

kubectl get pods -n nrem

The output should be similar to the following:

    
        NAME              READY   STATUS    RESTARTS   AGE
    nemo-ranker-text-reranking-nim-5f457975b6-tsxzz   1/1     Running   0          8m44s

Copied!

content_copy

NAME READY STATUS RESTARTS AGE nemo-ranker-text-reranking-nim-5f457975b6-tsxzz
1/1 Running 0 8m44s

Once we’ve verified the pods are running. We can test by enabling port
forwarding on the service to be able to access from our localhost:

    
        kubectl port-forward -n nrem service/nemo-ranker-text-reranking-nim 8000:8000

Copied!

content_copy

kubectl port-forward -n nrem service/nemo-ranker-text-reranking-nim 8000:8000

Next, we can open another terminal or tab in the cloud shell and try the
following request:

    
        curl -X 'POST' \
      'http://localhost:8000/v1/ranking' \
      -H 'accept: application/json' \
      -H 'Content-Type: application/json' \
      -d '{
        "query": {"text": "which way should i go?"},
        "model": "nvidia/llama-3.2-nv-rerankqa-1b-v2",
        "passages": [
          {
            "text": "two roads diverged in a yellow wood, and sorry i could not travel both and be one traveler, long i stood and     looked down one as far as i could to where it bent in the undergrowth;"
          },
          {
            "text": "then took the other, as just as fair, and having perhaps the better claim because it was grassy and wanted     wear, though as for that the passing there had worn them really about the same,"
          },
          {
            "text": "and both that morning equally lay in leaves no step had trodden black. oh, i marked the first for another     day! yet knowing how way leads on to way i doubted if i should ever come back."
          }
        ]
      }'

Copied!

content_copy

curl -X 'POST' \ 'http://localhost:8000/v1/ranking' \ -H 'accept:
application/json' \ -H 'Content-Type: application/json' \ -d '{ "query":
{"text": "which way should i go?"}, "model": "nvidia/llama-3.2-nv-
rerankqa-1b-v2", "passages": [ { "text": "two roads diverged in a yellow wood,
and sorry i could not travel both and be one traveler, long i stood and looked
down one as far as i could to where it bent in the undergrowth;" }, { "text":
"then took the other, as just as fair, and having perhaps the better claim
because it was grassy and wanted wear, though as for that the passing there
had worn them really about the same," }, { "text": "and both that morning
equally lay in leaves no step had trodden black. oh, i marked the first for
another day! yet knowing how way leads on to way i doubted if i should ever
come back." } ] }'

## Task 5. Deploying Milvus

The RAG application will leverage Milvus as the vector store - we’ll be
storing any embeddings generated for retrieval purposes in this vector store.
When we enable use of the knowledge base, the users query will be vectorized
using the embedding NIM and contents semantically similar will be retrieved
and injected as context into the inference NIM to improve the quality of the
generated output.

  1. **Create the Milvus Namespace**

In order to get started with the Milvus service, we create a namespace that
will be used to manage resources specific to the microservice:

    
        kubectl create namespace vectorstore

Copied!

content_copy

kubectl create namespace vectorstore

  2. **Add the Milvus Repository**

The Milvus microservice can be installed and managed via Helm charts. We can
add the Helm repo by running the following:

    
        helm repo add milvus https://zilliztech.github.io/milvus-helm/

Copied!

content_copy

helm repo add milvus https://zilliztech.github.io/milvus-helm/

And then updating the repository:

    
        helm repo update

Copied!

content_copy

helm repo update

  3. **Create Custom File to Utilize GPUs**

In order to leverage the Milvus microservice, we'll need to ensure that we
have access to at least one GPU resource. We can configure a file with custom
values, to request a GPU for the service:

    
        cat <<EOF > milvus_custom_value.yaml
    standalone:
      resources:
        requests:
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
    EOF

Copied!

content_copy

cat <<EOF > milvus_custom_value.yaml standalone: resources: requests:
nvidia.com/gpu: "1" limits: nvidia.com/gpu: "1" EOF

  4. **Install Milvus Helm Chart**

From here, we can install the Milvus Helm Chart and point to the file we
created in the previous section:

    
        helm install milvus milvus/milvus --set cluster.enabled=false --set etcd.replicaCount=1 --set minio.mode=standalone --set     pulsar.enabled=false -f milvus_custom_value.yaml -n vectorstore

Copied!

content_copy

helm install milvus milvus/milvus --set cluster.enabled=false --set
etcd.replicaCount=1 --set minio.mode=standalone --set pulsar.enabled=false -f
milvus_custom_value.yaml -n vectorstore

We can check the status of the pods, which should all be **up and running** in
a **Ready** state within a couple of minutes:

    
        kubectl get pods -n vectorstore

Copied!

content_copy

kubectl get pods -n vectorstore

Output should look like the following:

    
        NAME                                READY   STATUS    RESTARTS        AGE
    milvus-etcd-0                       1/1     Running   0               5m34s
    milvus-minio-76f9d647d5-44799       1/1     Running   0               5m34s
    milvus-standalone-9ccf56df4-m4tpm   1/1     Running   3 (4m35s ago)   5m34

Copied!

content_copy

NAME READY STATUS RESTARTS AGE milvus-etcd-0 1/1 Running 0 5m34s milvus-
minio-76f9d647d5-44799 1/1 Running 0 5m34s milvus-standalone-9ccf56df4-m4tpm
1/1 Running 3 (4m35s ago) 5m34

## Task 6. Deploying Chain Server & RAG Playground

Now that we’ve deployed all the prerequisite services, we can deploy the chain
server and RAG playground services.

  1. **Create the Chain Server & RAG Playground Namespace**

We can create a namespace for the chain server and RAG playground using the
following command:

    
        kubectl create namespace canonical-rag-langchain

Copied!

content_copy

kubectl create namespace canonical-rag-langchain

  2. **Installing the Helm Pipeline**

Both the chain server and RAG playground can be installed via Helm.  
First we fetch the Helm chart from NGC using our API key:

    
        helm fetch https://helm.ngc.nvidia.com/nvidia/aiworkflows/charts/rag-app-text-chatbot-langchain-24.08.tgz     --username='$oauthtoken' --password=$NGC_CLI_API_KEY

Copied!

content_copy

helm fetch https://helm.ngc.nvidia.com/nvidia/aiworkflows/charts/rag-app-text-
chatbot-langchain-24.08.tgz --username='$oauthtoken'
--password=$NGC_CLI_API_KEY

Once fetched, we can install the Helm pipeline by running the following:

    
        helm install canonical-rag-langchain rag-app-text-chatbot-langchain-24.08.tgz -n canonical-rag-langchain --set imagePullSecret.password=$NGC_CLI_API_KEY

Copied!

content_copy

helm install canonical-rag-langchain rag-app-text-chatbot-langchain-24.08.tgz
-n canonical-rag-langchain --set imagePullSecret.password=$NGC_CLI_API_KEY

We can check the status of the pods, which should all be **up and running** in
a **Ready** state within a couple of minutes:

    
        kubectl get pods -n canonical-rag-langchain

Copied!

content_copy

kubectl get pods -n canonical-rag-langchain

Output should look like the following:

    
        NAME                              READY   STATUS    RESTARTS   AGE
    chain-server-748bb5c5ff-58cw7     1/1     Running   0          71s
    rag-playground-855c7b9f65-qv42k   1/1     Running   0          71s

Copied!

content_copy

NAME READY STATUS RESTARTS AGE chain-server-748bb5c5ff-58cw7 1/1 Running 0 71s
rag-playground-855c7b9f65-qv42k 1/1 Running 0 71s

## Task 7. Access the RAG Playground Frontend Service

The RAG Playground service exposes a UI that enables interaction with the end
to end RAG pipeline. A user submits a prompt or a request and this triggers
the chain server to communicate with all the necessary services required to
generate output.  
We need to take a few steps in order to access the service.

  1. **Understanding Service Types**

The service type that’s been configured for the RAG playground is a `NodePort`
service type. With this service, kubernetes exposes applications to external
clients via specific ports on worker nodes.  
Given the exposure via ports on worker nodes, this service type is used purely
just for demo purposes and is **NOT** recommended for production workloads for
security related reasons.  
For more secure approaches, we recommend configuring services using
`ClusterIP` or `LoadBalancer` types.

We can verify the service types for the RAG playground by running the
following command:

    
        kubectl get svc -n canonical-rag-langchain

Copied!

content_copy

kubectl get svc -n canonical-rag-langchain

  2. **Accessing the Frontend Service**

In order to access the `NodePort` service, we need to configure a firewall
rule to allow TCP traffic on your node port. In order to do this, we need to
find the node port our applications are exposed on.  
We can do this using the following command:

    
        kubectl get service rag-playground --output yaml -n canonical-rag-langchain

Copied!

content_copy

kubectl get service rag-playground --output yaml -n canonical-rag-langchain

Output should look like the following:

    
        ...
      spec:
        ...
        ports:
        - nodePort: 30876
          port: 80
          protocol: TCP
          targetPort: 50000
        selector:
          app: metrics
          department: engineering
        sessionAffinity: None
        type: NodePort
    ...

Copied!

content_copy

... spec: ... ports: \- nodePort: 30876 port: 80 protocol: TCP targetPort:
50000 selector: app: metrics department: engineering sessionAffinity: None
type: NodePort ...

In this example, the node port the application is exposed on is **30876** ,
but this will be different depending on the application.

Using this node port value, we create a firewall to enable TCP traffic using
the following command:

_Note: Replace`NODE_PORT` from the command below, with the nodePort value from
the previous command_

    
        gcloud compute firewall-rules create test-node-port \
        --allow tcp:NODE_PORT

Copied!

content_copy

gcloud compute firewall-rules create test-node-port \ \--allow tcp:NODE_PORT

With this the firewall has been configured. In addition, we need the external
IP address of one of the nodes in the cluster. This can be done using the
following command:

    
        # get information on nodes in the cluster
    kubectl get nodes --output wide

Copied!

content_copy

# get information on nodes in the cluster kubectl get nodes --output wide

This output should yield information regarding the list of current nodes in
the cluster.  
Each node may come configured with an internal and external IP address. To
access the application, we get the external IP address the RAG playground
service is running on.

With both the external IP address and exposed node port, we can access the
frontend service in our browser using the following address:
`NODE_IP_ADDRESS:NODE_PORT/converse`.

From here, we should be able to interact with the service and get some outputs
from the LLM.

## Congratulations!

Congratulations! You've successfully deployed a RAG text Q&A agent
microservice on GKE using NVIDIA Inference Microservices. Now you can explore
enhancing your question-answering system by experimenting with different LLMs,
fine-tuning the embedding model, or scaling your deployment for increased
performance and availability.

NVIDIA offers NIMs with enterprise support through our GCP Marketplace
listing, [NVIDIA AI
Enterprise](https://console.cloud.google.com/marketplace/product/nvidia/nvidia-
ai-enterprise-vmi).

### Provide Feedback

Please take a moment to let us know what you think: [Feedback
Form](https://docs.google.com/forms/d/e/1FAIpQLSfpwZMFutvD3T2eR9VKQfHo6HWoDn-8U8D2a3_Srrwu2av1Pg/viewform?usp=pp_url&entry.738715088=Deploy+RAG+application+using+NVIDIA+NIMs+and+NeMo+Retriever+on+Google+Kubernetes+Engine)

### Learn More

Be sure to check out the following articles for more information:

  * [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode#why-standard)
  * [NVIDIA GPUs](https://cloud.google.com/compute/docs/gpus)
  * [NVIDIA AI Enterprise](https://console.cloud.google.com/marketplace/product/nvidia/nvidia-ai-enterprise-vmi)
  * [NVIDIA NIMs](https://www.nvidia.com/en-us/ai/)

### Google Cloud training and certification

...helps you make the most of Google Cloud technologies. [Our
classes](https://cloud.google.com/training/courses) include technical skills
and best practices to help you get up to speed quickly and continue your
learning journey. We offer fundamental to advanced level training, with on-
demand, live, and virtual options to suit your busy schedule.
[Certifications](https://cloud.google.com/certification/) help you validate
and prove your skill and expertise in Google Cloud technologies.

**Manual Last Updated August 20th, 2024**

**Lab Last TestedAugust 20th, 2024**

Copyright 2024 Google LLC All rights reserved. Google and the Google logo are
trademarks of Google LLC. All other company and product names may be
trademarks of the respective companies with which they are associated.

End Lab

####  Ready for more?

Here's another lab we think you'll like.

[ Lab Utilize the Streamlit Framework with Cloud Run and the Gemini API in
Vertex AI  0%  Continue
](https://explore.qwiklabs.com/focuses/7982?parent=catalog "Utilize the
Streamlit Framework with Cloud Run and the Gemini API in Vertex AI")

Lab instructions and tasks

  * [ISV026](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step1)
  * [Introduction](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step2)
  * [What you will learn](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step3)
  * [Learn the Components](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step4)
  * [Setup and Requirements](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step5)
  * [Task 1. Infrastructure Deployment](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step6)
  * [Task 2. Configure NVIDIA NGC API Key](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step7)
  * [Task 3. Deploying NVIDIA NIM](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step8)
  * [Task 4. Deploying NeMo Retriever Embedding Microservice](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step9)
  * [Task 5. Deploying NeMo Retriever Ranking Microservice](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step10)
  * [Task 5. Deploying Milvus](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step11)
  * [Task 6. Deploying Chain Server & RAG Playground](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step12)
  * [Task 7. Access the RAG Playground Frontend Service](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step13)
  * [Congratulations!](https://explore.qwiklabs.com/focuses/7867?catalog_rank=%7B%22rank%22%3A4%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=26820#step14)

#

Skip

Finish

![](./ragk_aks_files/start_lab-f45aca49782d4033c3ff688160387ac98c66941d.png)

#  Before you begin

  1. Labs create a Google Cloud project and resources for a fixed time
  2. Labs have a time limit and no pause feature. If you end the lab, you'll have to restart from the beginning.
  3. On the top left of your screen, click **Start lab** to begin

![](./ragk_aks_files/open_incognito_window-3ab2004605e88542dd7732361c3fc0e010c7fb6e.png)

#  Use private browsing

  1. Copy the provided **Username** and **Password** for the lab
  2. Click **Open console** in private mode

![](./ragk_aks_files/google_sign_in-f9eb84339d08f7db8c5fdfa7ce7726d3531dae80.png)

#  Sign in to the Console

  1. **Sign in** using your lab credentials. Using other credentials might cause errors or incur charges.
  2. **Accept** the terms, and skip the recovery resource page
  3. Don't click **End lab** unless you've finished the lab or want to restart it, as it will clear your work and remove the project

task_alt

# Score Details

OK

#

![](./ragk_aks_files/disable_lab-47cff1f6bc3610f58767251ff93c2f99cf4f9040.png)

This content is not currently available

We will notify you via email when it becomes available

[
](https://explore.qwiklabs.com/focuses/7867/set_up_lab_forward_url?parent=catalog)
No, thanks

Yes, notify me

#

![](./ragk_aks_files/notifcation_subscribed-2b4f9e51439e1a107488ab58a6e13c3c62ef8855.png)

Great!

We will contact you via email if it becomes available

[
](https://explore.qwiklabs.com/focuses/7867/set_up_lab_forward_url?parent=catalog)
Back to screen

#

![](./ragk_aks_files/open_incognito_window-3ab2004605e88542dd7732361c3fc0e010c7fb6e.png)

#  Setup your console before you begin

Use an Incognito or private browser window to run this lab. This prevents any
conflicts between your personal account and the Student account, which may
cause extra charges incurred to your personal account.

Continue anyway

Cancel

In this lab, you learn how to deploy a RAG application on GKE using NVIDIA NIM
and NeMo Retriever microservices

**Duration:** 0m setup · 300m access · 300m completion

**AWS Region:** [] ****

**Levels:** introductory

**Permalink:** <https://explore.qwiklabs.com/catalog_lab/18317>

Got It

# How satisfied are you with this lab?*

Cancel

Additional Comments

Additional Comments

Additional Comments

Submit

error_outline

# Are you sure? You may not be able to restart the lab, and you'll need to
start from the beginning if you do.

Cancel

End Lab

